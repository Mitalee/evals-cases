# Evals Demo: Progressive Prompt Improvement

An interactive Streamlit app demonstrating how iterative evaluation drives better AI responses through systematic prompt improvement.

## ğŸ¯ What This Demo Shows

Instead of guessing why AI responses fail, this app demonstrates:
1. **Run evaluations** with test questions
2. **Identify failures** through automated checks
3. **Improve system prompt** with targeted instructions
4. **Re-evaluate** to confirm improvement
5. **Repeat** until all evals pass

## ğŸƒ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run the App

```bash
streamlit run app.py
```

### 3. Get API Key

You'll need an Anthropic API key from [console.anthropic.com](https://console.anthropic.com)

## ğŸ“– The Story

### Meet Sarah
- 32-year-old professional
- Has a wedding in 2 weeks
- Needs a dress that fits well
- Usually between sizes (struggles with fit)
- Budget: $150

### Her 5 Questions

1. **Urgency Test**: "I have a wedding in 2 weeks... should I risk ordering it?"
2. **Demographic Analysis**: "What do other customers around my age think?"
3. **Personalization**: "I'm between M and L, which size should I order?"
4. **Risk Management**: "What's my backup plan if it doesn't work?"
5. **Value Assessment**: "Is this worth $120, or should I look elsewhere?"

### The Progressive Improvement Flow

**Question 1 â†’ Fails**
- AI gives generic advice
- Doesn't consider timeline
- Add instruction: "Consider time constraints and shipping times"
- Re-run â†’ Passes! âœ…

**Question 2 â†’ Fails**
- AI doesn't filter by age
- Doesn't analyze demographics
- Add instruction: "Analyze demographic data when asked about segments"
- Re-run â†’ Passes! âœ…

...and so on for all 5 questions!

## ğŸ® How to Use

1. **Start with Question 1**
   - Review Sarah's question
   - See the review context
   - Enter your API key
   - Click "Run Evaluation"

2. **Check Results**
   - See AI's response
   - Automatic evaluation shows pass/fail
   - Missing elements highlighted

3. **Improve When Failed**
   - Suggested prompt improvement shown
   - Click "Add to System Prompt"
   - Re-run evaluation
   - See the improvement!

4. **Progress Through All 5 Questions**
   - Each failure teaches a lesson
   - System prompt gets better incrementally
   - Final prompt is production-ready!

## ğŸ“Š What Gets Evaluated

For each response, we check if it includes:
- **Timeline consideration** (for urgency questions)
- **Demographic insights** (for segment questions)
- **Specific recommendations** (for personalization)
- **Risk mitigation** (for planning questions)
- **Value assessment** (for budget questions)

## ğŸ—ï¸ Project Structure

```
evals-demo/
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ claude_api.py       # API integration & evaluation logic
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ data/
â”‚   â””â”€â”€ evals_demo.db   # SQLite database with reviews
â””â”€â”€ README.md          # This file
```

## ğŸ“ Key Lessons

1. **Evals aren't just pass/fail** - they teach you what to improve
2. **System prompts are critical** - small changes â†’ big impact
3. **Iterate systematically** - fix one failure mode at a time
4. **Use real scenarios** - Sarah's questions expose real gaps
5. **Measure improvement** - quantify before/after

## ğŸš€ Deployment

This app can be deployed on Streamlit Cloud:

1. Push to GitHub
2. Connect to [streamlit.app](https://streamlit.io/cloud)
3. Deploy in one click!

Similar to: [GenZ Talk AI](https://genztalkai-mitalee.streamlit.app/)

## ğŸ¤ Contributing

Have ideas for better eval questions? Suggestions for prompt improvements?
Open an issue or PR!

## ğŸ“ License

MIT License

---

Built with â¤ï¸ by Mitalee | Powered by Claude & Streamlit
